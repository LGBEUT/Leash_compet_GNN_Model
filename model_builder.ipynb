{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76b9fc-240e-48e7-9df2-24584975717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains various utility functions for PyTorch build GNN model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_scatter import scatter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper\n",
    "# torch version of np unpackbits\n",
    "#https://gist.github.com/vadimkantorov/30ea6d278bc492abf6ad328c6965613a\n",
    "\n",
    "def tensor_dim_slice(tensor, dim, dim_slice):\n",
    "\treturn tensor[(dim if dim >= 0 else dim + tensor.dim()) * (slice(None),) + (dim_slice,)]\n",
    "\n",
    "# @torch.jit.script\n",
    "def packshape(shape, dim: int = -1, mask: int = 0b00000001, dtype=torch.uint8, pack=True):\n",
    "\tdim = dim if dim >= 0 else dim + len(shape)\n",
    "\tbits, nibble = (\n",
    "\t\t8 if dtype is torch.uint8 else 16 if dtype is torch.int16 else 32 if dtype is torch.int32 else 64 if dtype is torch.int64 else 0), (\n",
    "\t\t1 if mask == 0b00000001 else 2 if mask == 0b00000011 else 4 if mask == 0b00001111 else 8 if mask == 0b11111111 else 0)\n",
    "\t# bits = torch.iinfo(dtype).bits # does not JIT compile\n",
    "\tassert nibble <= bits and bits % nibble == 0\n",
    "\tnibbles = bits // nibble\n",
    "\tshape = (shape[:dim] + (int(math.ceil(shape[dim] / nibbles)),) + shape[1 + dim:]) if pack else (\n",
    "\t\t\t\tshape[:dim] + (shape[dim] * nibbles,) + shape[1 + dim:])\n",
    "\treturn shape, nibbles, nibble\n",
    "\n",
    "# @torch.jit.script\n",
    "def F_unpackbits(tensor, dim: int = -1, mask: int = 0b00000001, shape=None, out=None, dtype=torch.uint8):\n",
    "\tdim = dim if dim >= 0 else dim + tensor.dim()\n",
    "\tshape_, nibbles, nibble = packshape(tensor.shape, dim=dim, mask=mask, dtype=tensor.dtype, pack=False)\n",
    "\tshape = shape if shape is not None else shape_\n",
    "\tout = out if out is not None else torch.empty(shape, device=tensor.device, dtype=dtype)\n",
    "\tassert out.shape == shape\n",
    "\n",
    "\tif shape[dim] % nibbles == 0:\n",
    "\t\tshift = torch.arange((nibbles - 1) * nibble, -1, -nibble, dtype=torch.uint8, device=tensor.device)\n",
    "\t\tshift = shift.view(nibbles, *((1,) * (tensor.dim() - dim - 1)))\n",
    "\t\treturn torch.bitwise_and((tensor.unsqueeze(1 + dim) >> shift).view_as(out), mask, out=out)\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(nibbles):\n",
    "\t\t\tshift = nibble * i\n",
    "\t\t\tsliced_output = tensor_dim_slice(out, dim, slice(i, None, nibbles))\n",
    "\t\t\tsliced_input = tensor.narrow(dim, 0, sliced_output.shape[dim])\n",
    "\t\t\ttorch.bitwise_and(sliced_input >> shift, mask, out=sliced_output)\n",
    "\treturn out\n",
    "\n",
    "class dotdict(dict):\n",
    "\t__setattr__ = dict.__setitem__\n",
    "\t__delattr__ = dict.__delitem__\n",
    "\t\n",
    "\tdef __getattr__(self, name):\n",
    "\t\ttry:\n",
    "\t\t\treturn self[name]\n",
    "\t\texcept KeyError:\n",
    "\t\t\traise AttributeError(name)\n",
    "\n",
    "            \n",
    "print('helper ok!')\n",
    "\n",
    "# Setup hyperparameters\n",
    "PACK_NODE_DIM =9\n",
    "PACK_EDGE_DIM =1\n",
    "NODE_DIM =PACK_NODE_DIM*8\n",
    "EDGE_DIM =PACK_EDGE_DIM*8\n",
    "\n",
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.mlp_msg = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim + edge_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU()\n",
    "        )\n",
    "        self.mlp_upd = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, h, edge_index, edge_attr):\n",
    "        out = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, edge_attr):\n",
    "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
    "        return self.mlp_msg(msg)\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "\n",
    "    def update(self, aggr_out, h):\n",
    "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
    "        return self.mlp_upd(upd_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
    "\n",
    "class MPNNModel(nn.Module):\n",
    "    def __init__(self, num_layers=4, emb_dim=64, in_dim=9, edge_dim=4):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(in_dim, emb_dim)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #print(f\"Input batch.x shape: {batch.x.shape}\")\n",
    "        h = self.lin_in(F_unpackbits(batch.x,-1).float())  \n",
    "        #print(f\"Shape after lin_in: {h.shape}\")   \n",
    "        for conv in self.convs:\n",
    "            h = h + conv(h, batch.edge_index.long(), F_unpackbits(batch.edge_attr,-1).float())  # (n, d) -> (n, d)\n",
    "            #print(f\"Shape after conv layer: {h.shape}\")\n",
    "        h_graph = self.pool(h, batch.batch)\n",
    "        #print(f\"Shape after pooling: {h_graph.shape}\")  \n",
    "        return h_graph\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output_type = ['infer', 'loss']\n",
    "        graph_dim = 96\n",
    "        self.smile_encoder = MPNNModel(\n",
    "            in_dim=NODE_DIM, edge_dim=EDGE_DIM, emb_dim=graph_dim, num_layers=4,\n",
    "        )\n",
    "        self.bind = nn.Sequential(\n",
    "            nn.Linear(graph_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #print(f\"Initial batch.y shape: {batch.y.shape}\")\n",
    "        x = self.smile_encoder(batch)\n",
    "        bind = self.bind(x).squeeze(-1)\n",
    "        output = {}\n",
    "        #print(f\"Shape of bind: {bind.shape}, Shape of target before any change: {batch.y.shape}\")\n",
    "        if 'loss' in self.output_type:\n",
    "                       \n",
    "            target = batch.y.view(-1).float()\n",
    "            #print(f\"Shape of target used for loss: {target.shape}\")\n",
    "            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind, target.float())\n",
    "        if 'infer' in self.output_type:\n",
    "\n",
    "            probs = torch.sigmoid(bind)\n",
    "            output['bind'] = probs\n",
    "            output['preds'] = (probs >= 0.5).float()\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "100jML",
   "language": "python",
   "name": "100jml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
