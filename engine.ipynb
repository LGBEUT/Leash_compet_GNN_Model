{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84851e-55a1-4df2-8ef0-76c9cdc02c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_scatter import scatter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#### @title [RUN] Helper functions for managing experiments, training, and evaluating models.\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        #print(f\"Loaded y shape from batch (before moving to device): {batch.y.shape}\")\n",
    "        batch = batch.to(device)\n",
    "        #print(f\"Loaded y shape from batch (after moving to device): {batch.y.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        #print(f\"Forme de batch.y dans train: {batch.y.shape}\")  # Imprimer la forme de batch.y\n",
    "        #print(f\"Forme des logits (bind) : {output['bind'].shape}, Forme de la cible (y) : {batch.y.shape}\")\n",
    "        loss = output['bce_loss']\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * batch.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval(model, loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0  # Initialiser total_correct\n",
    "    total = 0\n",
    "\n",
    "    for batch  in loader:\n",
    "        batch  = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(batch)\n",
    "            predictions = output['preds']\n",
    "            correct = (predictions == batch.y).sum().item()\n",
    "            total_correct += correct\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "    accuracy =  total_correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(model, model_name, train_loader, val_loader, test_loader, n_epochs=100):\n",
    "    \n",
    "    print(f\"Running experiment for {model_name}, training on {len(train_loader.dataset)} samples for {n_epochs} epochs.\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"\\nModel architecture:\")\n",
    "    print(model)\n",
    "    total_param = 0\n",
    "    for param in model.parameters():\n",
    "        total_param += np.prod(list(param.data.size()))\n",
    "    print(f'Total parameters: {total_param}')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Adam optimizer with LR 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # LR scheduler which decays LR when validation metric doesnt improve\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.9, patience=5, min_lr=0.00001)\n",
    "    \n",
    "    print(\"\\nStart training:\")\n",
    "    best_val_accuracy = 0  # Initialise to 0 to avoid None comparison\n",
    "    corresponding_test_accuracy = 0  # Ajout de la variable pour stocker la meilleure test_accuracy\n",
    "    \n",
    "    train_losses = []  # Liste pour stocker les pertes dentraînement\n",
    "    val_accuracies = []  # Liste pour stocker les precisions de validation\n",
    "    test_accuracies = []  # Liste pour stocker les precisions de test\n",
    "    perf_per_epoch = []  # Track performance per epoch\n",
    "    t = time.time()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # Call LR scheduler at start of each epoch\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Train model for one epoch, return avg. training loss\n",
    "        avg_epoch_loss = train(model, train_loader, optimizer, device)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        # Evaluate model on validation set\n",
    "        val_accuracy = eval(model, val_loader, device)\n",
    "        val_accuracies.append(val_accuracy)  \n",
    "        \n",
    "        # Si la précision de validation sameliore, evaluation sur lensemble de test\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            corresponding_test_accuracy = eval(model, test_loader, device)\n",
    "            \n",
    "        test_accuracies.append(corresponding_test_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Print and track stats every 10 epochs\n",
    "            print(f'Epoch: {epoch:03d}, LR: {lr:.6f}, Loss: {avg_epoch_loss:.7f}, Val Accuracy: {val_accuracy:.7f}, Test Accuracy: {corresponding_test_accuracy:.7f}')\n",
    "        \n",
    "        scheduler.step(val_accuracy)\n",
    "        perf_per_epoch.append((corresponding_test_accuracy, val_accuracy, epoch, model_name))\n",
    "    \n",
    "    t = time.time() - t\n",
    "    train_time = t/60\n",
    "    print(f\"\\nDone! Training took {train_time:.2f} mins. Best validation accuracy: {best_val_accuracy:.7f}, corresponding test accuracy: {corresponding_test_accuracy:.7f}.\")\n",
    "    # Tracer les graphiques\n",
    "    epochs = range(1, n_epochs + 1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Graphique des pertes dentrainement\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Graphique des precisions de validation\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Graphique des precisions de test\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, test_accuracies, 'g', label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_val_accuracy, corresponding_test_accuracy, train_time, perf_per_epoch\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "100jML",
   "language": "python",
   "name": "100jml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
